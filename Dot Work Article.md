# Dot Work Article


- So according to the task
I started working by reading the docs of dot, looking around the test env and also dabstep repo
- I setup Claude Code and automated the setup to fetch all dabstep data and questions from hugging face and link with dot client
- I then automated running a fake dot client and get a score matcher according to dabstep repo and check the linkage
- I the proceeded to create connection with the dot env and getting first-hand answers from agentic mode on dev split of dab step
- I afced trouble due to lock system of duckdb when uploading multi files, which resulted in a delay
- proceeded to get a score of 1/10 , initial check worked well
- Then I worked with claude and chatgpt to create  an iterative loop to get teh best out of dot and its context manager, basically create custom derived data sources and generalized instructions based on dab step manuals and taht improved the score to 4/10 on dev split
- Then i worked on analysing the dataset myself, figuring out cases where which semantics are used
clearly fee questions required a intracountry sematic of three different types which dot was missing, and the data sources was not converted properly, i re-uploaded the fixed data sources and updated the context of the data source instead of calculating at run time to be stored already and then fixed basic issues like the mismatch of dot not understanding rules on db questions such as highest fraud rate , dot was calculating db just by total fraud count metric meanwhile according to the manual it needed to be volume based, so talking to the agent and also adding a prompt focus on it, helped cracking the fraud specific questions
- Then i worked upon the other focused areas where intracountry semantics were again causing dot an issue on the questions, dot was assuming [], false, true as only two different types whereas in the manual it was specifically mentioned to treat [] semantic as wildcards and others as 0,1 boolean (to b included in questionnaire or not), so talking the context agent about this and trying to fix the table description and org notes helped improve dots accuracy there, which was a result of interacting with dot analysis mode and seeing its full log to identify the discrepancy between dot’s answer and ground truth.
- Here i faced a small issue with the context agent where I ran into merge conflicts but was not able to figure out how to manually resolve them ,later I started getting the option for the context agent itself to resolve it, which is a great customer support but another manual tooling may have helped further to analyse better
- I added in the checker types of wrong answer whether it was a sql issue, formate error or just wrong answer, i wanted to include dot raw response ,but i think also adding it logs amy have helped in anlaysing better and reducing time spent on runs
- These small conversations with dot and trying to improve its structure resulted in me getting a score of 8/10 on dev split consistently, with sometimes 2/10 timing out or 1/10 timing out, even on dot analysis mode chat, I think the score max achievable was 9/10 as one of the questions for E value on matching fee id was not able to achieve even though i had improved dot’s accuracy to identify the correct account type
- So initial 30 score submission ended up me getting - 2.78 % on easy and 4.2 % on hard
- Based on assumptions of the 30 score time outs and waits, i increased the time out to 90 minutes per question made an async runner to just run 5 workers and keep polling( reduced to 2 later for lower load but still faced time out issues) , I assumed the questions which are taking longer time require more to them so sat down individually with dot for 4 questions on analysis mode and tried to figure out its logic and where it maybe going wrong, figured out its mistakes on fee id matching from db and the utilized the context agent to read the chat and optimize the organisation notes and table descriptions for dot to improve its accuracy and speed, when i focused on the strict fee ruling, then the context answers seemed to improve. for further security i added that t the prompt as well
- After multiple attempts i ended up getting an improved score of 4.5 % on hard same score of 2.78 % on easy from the full submission
- After more analysis with dot chat i was able to try and improve tne notes with context agent but ended up decreasing the score from 4.5 on hard to 4.23% on hard back again. Here i faced an issue with context agent as i noticed its using  git version control to maintain itself so i wanted to revert back to my previous committed changes so that i can try and get the score back an focus on improving again, but even after multiple attempts the agent was unable to rollback properly to its previous commit.
- Further trial and effort is what’s required to improve dot’s score and understanding, if i am able to explain dot the business logic well and wildcard exemptions well enough it should improve the hard score further and also focus on optimizing and adding more derived data source for dot to be quicker and less on time outs , which i did similar to monthly merchant metric, which dot otherwise had to derive from merchants data source